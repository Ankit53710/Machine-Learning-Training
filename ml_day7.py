# -*- coding: utf-8 -*-
"""ML day7

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z8PD2yhCQCdNsSpIiE-cNGjv_3GXwoY5

**NLP**

RNN(recurrent neural network)(help to learn sequence) types:-

1 to 1

1 to many

many to 1

many to many

many to many
"""

#long short term memory
from google.colab import files
uploaded=files.upload()

import pandas as pd
df=pd.read_excel(uploaded['time.xlsx'])
data=df['time']
data1=data.tolist()
data1

def preparedata1(data,steps):
  x=[]
  y=[]
  while True:
    for i in range(len(data)):
      b=i+steps+1
      for j in range(i,b):
        if j == len(data1)-1:
          return x,y
        if j<b:
          x.append(data1[j])
      else:
         y.append(data1[j])
x1,y1=preparedata1(data1,3)
y1

def preparedata(data,steps):
  x1=[]
  y1=[]
  for i in range(len(data)-steps):
    x1.append(data[i:i+3])
    y1.append(data[i+3])
  return x1,y1
x1,y1=preparedata(data1,3)
import numpy as np
xarr=np.array(x1)
yarr=np.array(y1)

from keras.layers import Dense,LSTM
from keras.models import Sequential
model=Sequential()
model.add(LSTM(200,activation='relu',return_sequences=True,input_shape=(3,1)))
model.add(LSTM(200))
model.add(Dense(42,activation='softmax'))
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
o=model.fit(xarr,yarr,epochs=50)

"""**Sentimental analysis**"""

#word embedding
sentences=["this is my favourite restaurant",
           "i heat her","food is ok"]
sentiment=['positive','negative','neutral']

import nltk #netural language tool kit
nltk.download('punkt')
from nltk.tokenize import word_tokenize
words=[]
for i in sentences:
  p=word_tokenize(i)
  print(p)

!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip #word embedding

!unzip glove.6B.zip

def add_to_dict(filename,d):
  import numpy as np
  with open(filename,'r') as f:
    for line in f.readlines():
      l=line.split(' ')
      d[l[0]]=np.array(l[1:],dtype='float')

dics={}
add_to_dict('glove.6B.50d.txt',dics)



import nltk
# pragraphs to sentence to word to base form remove unuse word then sentence then padding
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.stem import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()

def message_to_tokenlist(s):
  tokens=nltk.word_tokenize(s)
  lower_case=[t.lower() for t in tokens]
  lemm=[lemmatizer.lemmatize(t) for t in lower_case]
  usefull_tokens=[t for t in lemm if t in dics]
  return usefull_tokens

message_to_tokenlist('i am so HAPPY at sisTec')

def message_to_vector(s,dics):
  usefull_token=message_to_tokenlist(s)
  vector=[]
  for token in usefull_token:
    if token not in dics:
      continue
    vector.append(dics[token])
  import numpy as np
  return np.array(vector)

a=message_to_vector('hello how do you do',dics)
a.shape

#data frame banne ja rhe h
import pandas as pd
data=[["the restaurant is awsome",1],["the food is so oily and spicy",0],["food is ok",1]]
df=pd.DataFrame(data,columns=['review','sentiment'])
df

def df_to_x_y(df,dics):
  x=[]
  y=df['sentiment'].to_numpy().astype('int')
  for message in df['review']:
    message_in_matrix=message_to_vector(message,dics)
    x.append(message_in_matrix)
  return x,y

x,y=df_to_x_y(df,dics)

def pad_x(x,length):#lenght max length of sentence
  x_copy=[]
  for i in x:
    difference=length-i.shape[0]
    pad=np.zeros(shape=(difference,50))
    x_copy.append(np.concatenate([i,pad]))
  return np.array(x_copy)

xarr=pad_x(x,7)
xarr.shape

from keras.models import Sequential
from keras.layers import LSTM,Dense,Dropout,Input #dropout use for remove over fitting,imdm dataset
model.add(Input(shape=(7,50)))
model=Sequential()
model.add(LSTM(64,activation='relu',return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(64,activation='relu',return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

model.fit(xarr,y,epochs=50)

o=model.predict(xarr)
o>0.5

